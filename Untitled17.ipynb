{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "KxJ5_JZSA4kO"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report, accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('spam.csv', sep='\\t', header=None, names=['label', 'message'], encoding='latin1')"
      ],
      "metadata": {
        "id": "-Ei2yk0LBNTp"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    if not isinstance(text, str):\n",
        "        return []  # Return empty list for non-string (e.g., NaN) values\n",
        "\n",
        "    # 1. Lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # 2. Tokenize\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # 3. Remove punctuation and stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word not in stop_words and word not in string.punctuation]\n",
        "\n",
        "    return tokens\n"
      ],
      "metadata": {
        "id": "6o7-AHYECscz"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Loading dataset\n",
        "df = pd.read_csv('spam.csv', sep='\\t', header=None, encoding='latin1')\n",
        "\n",
        "# Manually split the first column into 'label' and 'message'\n",
        "df[['label', 'message']] = df[0].str.split(',', n=1, expand=True)\n",
        "\n",
        "# Drop the original combined column\n",
        "df = df.drop(columns=[0])\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess_text(text):\n",
        "    if not isinstance(text, str):\n",
        "        return []  # Return empty list for non-string (e.g., NaN) values\n",
        "\n",
        "    # 1. Lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # 2. Tokenize\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # 3. Remove punctuation and stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word not in stop_words and word not in string.punctuation]\n",
        "\n",
        "    return tokens\n",
        "\n",
        "# Apply preprocessing to each message\n",
        "df['tokens'] = df['message'].apply(preprocess_text)\n",
        "\n",
        "# View the first few rows\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lzq-7WhQB75t",
        "outputId": "bbb3bdf4-70d0-4f50-c6e8-d7da4868aef7"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  label                                            message  \\\n",
            "0    v1                                              v2,,,   \n",
            "1   ham  \"Go until jurong point, crazy.. Available only...   \n",
            "2   ham                   Ok lar... Joking wif u oni...,,,   \n",
            "3  spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
            "4   ham  U dun say so early hor... U c already then say...   \n",
            "\n",
            "                                              tokens  \n",
            "0                                               [v2]  \n",
            "1  [``, go, jurong, point, crazy, .., available, ...  \n",
            "2           [ok, lar, ..., joking, wif, u, oni, ...]  \n",
            "3  [free, entry, 2, wkly, comp, win, fa, cup, fin...  \n",
            "4  [u, dun, say, early, hor, ..., u, c, already, ...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gensim\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MvmMi4TeDSe3",
        "outputId": "e13af627-58eb-42a0-9c6f-7e2113fb07fd"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "word2vec_model = api.load(\"word2vec-google-news-300\")"
      ],
      "metadata": {
        "id": "lqJR0rAID-8e"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_avg_word2vec(tokens, model, vector_size=300):\n",
        "    valid_vectors = [model[word] for word in tokens if word in model]\n",
        "    if valid_vectors:\n",
        "        return np.mean(valid_vectors, axis=0)\n",
        "    else:\n",
        "        return np.zeros(vector_size)  # Return a zero vector if none are in vocab\n",
        "\n",
        "# Appling dataset\n",
        "df['embedding'] = df['tokens'].apply(lambda tokens: get_avg_word2vec(tokens, word2vec_model))\n"
      ],
      "metadata": {
        "id": "SDvEZU8oGR07"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df[df['label'].isin(['spam', 'ham'])]  # Remove anything unexpected\n",
        "df = df.dropna(subset=['label', 'embedding'])  # Drop missing labels/embeddings"
      ],
      "metadata": {
        "id": "yRDdtdO7Ix5G"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preparing feature matrix (X) and labels (y)\n",
        "X = np.stack(df['embedding'].values)  # 2D array of shape (n_samples, 300)\n",
        "\n",
        "# Encode 'spam' = 1, 'ham' = 0\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(df['label'])  # Converts ['ham', 'spam'] â†’ [0, 1]\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train logistic regression model\n",
        "clf = LogisticRegression(max_iter=1000)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "#  Evaluate\n",
        "y_pred = clf.predict(X_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jw4-YnwvGi5_",
        "outputId": "4a108ebd-1267-43e8-df6d-6179a66bcf02"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9390134529147982\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ham       0.95      0.98      0.97       965\n",
            "        spam       0.83      0.69      0.75       150\n",
            "\n",
            "    accuracy                           0.94      1115\n",
            "   macro avg       0.89      0.83      0.86      1115\n",
            "weighted avg       0.94      0.94      0.94      1115\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to preprocess and vectorize a single message\n",
        "def preprocess_and_vectorize(message, w2v_model, vector_size=300):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    # Lowercase and tokenize\n",
        "    tokens = word_tokenize(message.lower())\n",
        "\n",
        "    # Remove stopwords and punctuation\n",
        "    tokens = [word for word in tokens if word not in stop_words and word not in string.punctuation]\n",
        "\n",
        "    # Get average Word2Vec vector\n",
        "    valid_vectors = [w2v_model[word] for word in tokens if word in w2v_model]\n",
        "    if valid_vectors:\n",
        "        return np.mean(valid_vectors, axis=0)\n",
        "    else:\n",
        "        return np.zeros(vector_size)\n",
        "\n",
        "# Main prediction function\n",
        "def predict_message_class(model, w2v_model, message):\n",
        "    vec = preprocess_and_vectorize(message, w2v_model)\n",
        "    vec = vec.reshape(1, -1)  # Reshape for sklearn input\n",
        "    prediction = model.predict(vec)[0]\n",
        "    return 'spam' if prediction == 1 else 'ham'\n"
      ],
      "metadata": {
        "id": "fG4h75QKHdeo"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_message_class(clf, word2vec_model, \"win a dollar.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Ft-tOz2yHvnY",
        "outputId": "25126b94-3156-49c0-eca1-1f4d9a0cbf32"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'spam'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    }
  ]
}